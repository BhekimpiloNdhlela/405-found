It is common knowledge that images are stored on computers as matrices of pixels. Each pixel has its own RGB (red, green, blue) value which indicates the colours present in the pixel. Computers are able to traverse pixels in an image and locate specific pixels, based on their RGB values.  For x-ray images we consider greyscale images, where each pixel has a greyscale value \cite{9}.\newline
\\
The algorithm starts at the top left pixel and moves across the row and down the columns analysing each pixel using these greyscale values. If it is looking for a pixel with a particular greyscale value it will calculate the difference between the input greyscale value and the one it is looking for. The algorithm then returns the pixels, highlighting the ones that were closest to the requested greyscale value. \newline
\\
If the algorithm is looking for something bigger than a pixel, it has to search for and identify groups of pixels called patches. For any computer vision system, it is important to identify edges. The algorithm is able to detect edges by detecting a change in greyscale values that persists in a particular direction. Consider a pixel on an edge in an image. The algorithm will be able to detect a significant colour difference on either side of the pixel. For vertical edges, it will find a large difference in the greyscale values of the pixels on its left and on its right \cite{9}.\newline 
\\
Mathematically, a matrix called a kernel or filter is used. The kernel for detecting vertical edges can be as simple as a $3\times3$ matrix, consisting of a column of negative ones, a column of zeros and a column of positive ones \cite{9}. Essentially, it performs multiplication on the pixel and the surrounding pixels. It then sums all nine values, calculating the difference between pixels to the left and to the right of the pixel.\newline
\\
Finally, it replaces the pixel’s greyscale value with a new value indicating the contrast or difference between the pixels to its left and to its right. After this replacement of pixels (or filtering) the highest pixel values indicate the location of an edge in the image \cite{9}.\newline
\\
This process of applying a kernel to each pixel in a group of pixels is called a convolution \cite{9} For the detection of horizontal edges, a different kernel is used. Kernels have many uses in computer vision. They are not only able to detect edges in different directions, but they are also used to sharpen images, to blur them, to detect islands of contrast\cite{9}, to recognise patterns and assign them to objects, such as noses or eyes. \newline
\\
Convolutional neural networks (CNN's) can learn their own kernels and recognise feature in images. CNN's are able to process image data by using groups of neurons. A single input image can be passed through multiple neurons which each detect a unique patch that corresponds to a pattern or texture found in the image. The neurons then output stacks of images that are called filters. This is called a convolutional layer \cite{9}. The outputs can then be processed by another set of neurons or another layer.\newline
\\
Convolutional neural networks can consist of many layers. For example, the first layer could detect edges, while the second layer detects shapes and the third detects objects and so on. The final layer is called the fully connected layer and it is usually the one to classify images. CNNs may be many layers deep to classify complex images\cite{9}. Deep learning is the term used when deep convolutional neural networks are used in machine learning.  This is useful for analysing images for patterns and for considering the ways in which these patterns occur with respect to each other \cite{9}. When used on images of objects on and inside the human body, deep convolutional neural networks are able to recognise biometric data. This is used in facial recognition.\newline
\\
Classification of an image requires four layers. These layers are the convolutional layer, the ReLU layer, the pooling layer and finally the fully connected layer \cite{10}. 
For example, consider a convolutional neural network that classifies images as X’s or O’s. The classification images would be images of a perfect X and a perfect O. These are the image that the network initially learns on. During training, the network will compare any input image to this classification image. The network will first apply each layer to the classification images and output classification image results. For any input image it will also apply each layer to the input image and compare its output to the output of the classification images.\newline
\\
The convolutional layer compares patterns mathematically. In this layer one image becomes a stack of filtered images. The pixels are assigned new values indicating to what extent the pixel and its surrounding pixels is similar to the pattern that the algorithm is searching for. Each filtered image is simply an array of values, specifying whether and where the patterns occur. This process is done for the classification images. \newline
\\
The Rectified Liner Units (ReLU) layer takes the filtered images as input and changes any negative values in the pixels to zeros but leaves the positive values unchanged \cite{9}. This is to correct for any mathematical errors that could occur later due to the summing of negative values.\newline
\\
The pooling layer shrinks the filtered image by replacing windows of pixels by a single pixel with the highest value that occurred in the window. It is essentially a method of compressing the filtered image without losing too much information. The window size and slide size should be chosen according to the extent of compression desired.\newline
\\
The convolutional, the ReLU and the pooling layers can be stacked up to filter and compress images until they have been reduced to a size that is small enough to be analyzed easily \cite{11}. \newline
\\
The final classification takes place at the fully connected layer. This layer organises all the values into a single list. Each classification image has a unique list. Each list has high values at particular positions which is unique to the classification \cite{10}. The list created for any input image is compared to the list of each classification image. The similarity between the list of the input image and the lists of the classification images is calculated based on the positions of the highest values in the lists. The input image list is then assigned values indicating the similarity to each classification image’s list. The final classification is made based on the highest similarity.\newline
\\
Analysts are able to decide whether the neural network performed the correct classification. If they find that it was incorrect, they calculate the error in the prediction. Subsequently, they will carry out a weight adjustment to decrease errors in the future \cite{11}.\newline
\\
There are other techniques for analysts to enhance the performance of a neural network. These are machine learning techniques which will be discussed later.
